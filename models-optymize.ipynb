{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -r ../input/requirements/requirements.txt\n!pip install -q grape -U\n!pip install -q plot_keras_history seedir silence_tensorflow\n!pip install -q tsnecuda==3.0.0+cu110 -f https://tsnecuda.isx.ai/tsnecuda_stable.html --no-dependencies\n!pip install -q MulticoreTSNE\n!pip install -q faiss\n!pip install keras_bed_sequence keras_mixed_sequence -U\n!pip install extra_keras_metrics\n!pip install keras-tuner\n!pip install loguru\n!pip install barplots\n!pip install epigenomic_dataset>=1.1.7","metadata":{"id":"8_EXTbCcK4b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# DIRECTORY\n# ===============\n\nTUNER_DIR = \"../input/tuners/tuners/tuner_hp_\" \n\nMODEL_TYPE_FFNN = \"ffnn\"\nMODEL_TYPE_CNN = \"cnn\"\nMODEL_TYPE_MMNN = \"mmnn\"\n\nTUNER_DIR_FFNN = TUNER_DIR+MODEL_TYPE_FFNN\nTUNER_DIR_CNN = TUNER_DIR+MODEL_TYPE_CNN\nTUNER_DIR_MMNN = TUNER_DIR+MODEL_TYPE_MMNN\n\nTUNER_PROJECT_NAME_FFNN = \"bioinformatics_project_ffnn_hp\"\nTUNER_PROJECT_NAME_CNN = \"bioinformatics_project_cnn_hp\"\nTUNER_PROJECT_NAME_MMNN = \"bioinformatics_project_mmnn_hp\"\n\nMODELS_TYPE = [MODEL_TYPE_FFNN, MODEL_TYPE_CNN, MODEL_TYPE_MMNN]\n\nHP_MAX_EPOCHS_FFNN = 30\nHP_MAX_EPOCHS_CNN = 35\nHP_MAX_EPOCHS_MMNN = 25\n\n# ===============\n# DATA\n# ===============\n\nHOLDOUTS_NUM_SPLIT = 10\nTEST_SIZE = 0.2\n\n# ===============\n# EPIGENOMIC DATA\n# ===============\n\nWINDOW_SIZE = 256\nCELL_LINE = \"H1\"\nGENOME_CACHE_DIR = \"./bio_data/genomes\"\n\n# ===============\n# FFNN\n# ===============\n\nFFNN_NAME_HP = \"ffnn_hp\"\nFFNN_NAME = \"BinaryClassificationFFNN\"\n\n# ===============\n# CNN\n# ===============\n\nCNN_NAME_HP = \"ffnn_hp\"\nCNN_NAME = \"BinaryClassificationCNN\"\n\n# ===============\n# MMNN\n# ===============\n\nMMNN_NAME_HP = \"mmnn_hp\"\nMMNN_SIMPLE = \"MMNN\"\nMMNN_BOOST = \"BoostedMMNN\"\n","metadata":{"id":"wjZXS8dqWf4j","executionInfo":{"status":"ok","timestamp":1640104292246,"user_tz":-60,"elapsed":31,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T16:51:30.295728Z","iopub.execute_input":"2021-12-24T16:51:30.296047Z","iopub.status.idle":"2021-12-24T16:51:30.304579Z","shell.execute_reply.started":"2021-12-24T16:51:30.296005Z","shell.execute_reply":"2021-12-24T16:51:30.303823Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\nfrom keras_mixed_sequence import MixedSequence, VectorSequence\nfrom keras_bed_sequence import BedSequence\nfrom cache_decorator import Cache\nfrom multiprocessing import cpu_count\nfrom ucsc_genomes_downloader import Genome\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler\n\nimport pandas as pd\nimport numpy as np\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom typing import Dict, List, Tuple, Optional, Union\n\nfrom pathlib import Path\nfrom datetime import datetime\nimport time\n\n\ndef get_cnn_sequence(\n        genome: Genome,\n        bed: pd.DataFrame,\n        y: np.ndarray,\n        batch_size: int = 1024\n) -> MixedSequence:\n    \"\"\"Returns sequence to train a CNN model on genomic sequences.\n\n    Implementative details\n    -------------------------\n    This sequence can be used for either binary classification or\n    for regresssion, just change the y accordingly.\n\n    Parameters\n    -------------------------\n    genome: Genome,\n        The genome from where to extract the genomic sequence.\n    bed: pd.DataFrame,\n        The BED file coordinates describing where to extract the sequences.\n    y: np.ndarray,\n        The values the model should predict.\n    batch_size: int = 1024,\n        The size of the batches to generate\n\n    Returns\n    --------------------------\n    MixedSequence object to train a CNN.\n    \"\"\"\n    return MixedSequence(\n        x={\n            \"input_sequence_data\": BedSequence(\n                genome,\n                bed,\n                batch_size=batch_size,\n            )\n        },\n        y=VectorSequence(\n            y,\n            batch_size=batch_size\n        )\n    )\n\n\ndef get_ffnn_sequence(\n        X: np.ndarray,\n        y: np.ndarray,\n        batch_size: int = 1024\n) -> MixedSequence:\n    \"\"\"Returns sequence to train a FFNN model on epigenomic data.\n\n    Implementative details\n    -------------------------\n    This sequence can be used for either binary classification or\n    for regresssion, just change the y accordingly.\n\n    Parameters\n    -------------------------\n    X: np.ndarray,\n        The vector from where to extract the epigenomic data.\n    y: np.ndarray,\n        The values the model should predict.\n    batch_size: int = 1024,\n        The size of the batches to generate\n\n    Returns\n    --------------------------\n    MixedSequence object to train a FFNN.\n    \"\"\"\n    return MixedSequence(\n        x={\n            \"input_epigenomic_data\": VectorSequence(\n                X,\n                batch_size\n            )\n        },\n        y=VectorSequence(\n            y,\n            batch_size=batch_size\n        )\n    )\n\n\ndef get_mmnn_sequence(\n        genome: Genome,\n        bed: pd.DataFrame,\n        X: np.ndarray,\n        y: np.ndarray,\n        batch_size: int = 1024\n) -> MixedSequence:\n    \"\"\"Returns sequence to train a MMNN model on both genomic sequences and epigenomic data.\n\n    Implementative details\n    -------------------------\n    This sequence can be used for either binary classification or\n    for regresssion, just change the y accordingly.\n\n    Parameters\n    -------------------------\n    genome: Genome,\n        The genome from where to extract the genomic sequence.\n    bed: pd.DataFrame,\n        The BED file coordinates describing where to extract the sequences.\n    X: np.ndarray,\n        The vector from where to extract the epigenomic data.\n    y: np.ndarray,\n        The values the model should predict.\n    batch_size: int = 1024,\n        The size of the batches to generate\n\n    Returns\n    --------------------------\n    MixedSequence object to train a MMNN.\n    \"\"\"\n    return MixedSequence(\n        x={\n            \"input_sequence_data\": BedSequence(\n                genome,\n                bed,\n                batch_size=batch_size,\n            ),\n            \"input_epigenomic_data\": VectorSequence(\n                X,\n                batch_size\n            )\n        },\n        y=VectorSequence(\n            y,\n            batch_size=batch_size\n        )\n    )\n\n\n@Cache(\n    cache_path=[\n        \"./boruta/kept_features_{_hash}.json\",\n        \"./boruta/discarded_features_{_hash}.json\"\n    ],\n    args_to_ignore=[\"X_train\", \"y_train\"]\n)\ndef execute_boruta_feature_selection(\n        X_train: pd.DataFrame,\n        y_train: np.ndarray,\n        holdout_number: int,\n        task_name: str,\n        max_iter: int = 100\n):\n    \"\"\"Returns tuple with list of kept features and list of discared features.\n    \n    Parameters\n    --------------------------\n    X_train: pd.DataFrame,\n        The data reserved for the input of the training of the Boruta model.\n    y_train: np.ndarray,\n        The data reserved for the output of the training of the Boruta model.\n    holdout_number: int,\n        The current holdout number.\n    task_name: str,\n        The name of the task.\n    max_iter: int = 100,\n        Number of iterations to run Boruta for.\n\n    Returns\n    -------\n    kept_features: list(),\n        List of indices referring to the features to be maintained.\n    discarded_features: list(),\n        List of indices referring to the features to be eliminated.\n    \"\"\"\n\n    model = RandomForestClassifier(n_jobs=cpu_count(), class_weight='balanced_subsample', max_depth=5)\n\n    # Create the Boruta model\n    boruta_selector = BorutaPy(\n        model,  # Defining the model that Boruta should use.\n        n_estimators='auto',  # We leave the number of estimators to be decided by Boruta.\n        verbose=False,\n        alpha=0.05,  # p_value\n        # In practice one would run at least 100-200 times,\n        # until all tentative features are exausted.\n        max_iter=max_iter,\n        random_state=42,\n    )\n    # Fit the Boruta model\n    boruta_selector.fit(X_train.values, y_train)\n\n    # Get the kept features and discarded features\n    kept_features = list(X_train.columns[boruta_selector.support_])\n    discarded_features = list(X_train.columns[~boruta_selector.support_])\n\n    # Filter out the unused featured.\n    return kept_features, discarded_features\n\n\ndef to_bed(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Return bed coordinates from given dataset.\"\"\"\n    return data.reset_index()[data.index.names]\n\ndef normalize_epigenomic_data(\n    train_x: np.ndarray,\n    test_x: np.ndarray = None\n) -> Tuple[np.ndarray]:\n    \"\"\"Return imputed and normalized epigenomic data.\n\n    We fit the imputation and normalization on the training data and\n    apply it to both the training data and the test data.\n\n    Parameters\n    -------------------------\n    train_x: np.ndarray,\n        Training data to use to fit the imputer and scaled.\n    test_x: np.ndarray = None,\n        Test data to be normalized.\n\n    Returns\n    -------------------------\n    Tuple with imputed and scaled train and test data.\n    \"\"\"\n    # Create the imputer and scaler object\n    imputer = KNNImputer()\n    scaler = RobustScaler()\n    # Fit the imputer object\n    imputer.fit(train_x)\n    # Impute the train and test data\n    imputed_train_x = imputer.transform(train_x)\n    if test_x is not None:\n        imputed_test_x = imputer.transform(test_x)\n    # Fit the scaler object\n    scaler.fit(imputed_train_x)\n    # Scale the train and test data\n    scaled_train_x = scaler.transform(imputed_train_x)\n    if test_x is not None:\n        scaled_test_x = scaler.transform(imputed_test_x)\n    if test_x is not None:\n        # Return the normalized data\n        return scaled_train_x, scaled_test_x\n    return scaled_train_x","metadata":{"id":"0_C1FOVRXLct","executionInfo":{"status":"ok","timestamp":1640104429257,"user_tz":-60,"elapsed":4788,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T16:51:30.306111Z","iopub.execute_input":"2021-12-24T16:51:30.306373Z","iopub.status.idle":"2021-12-24T16:51:35.969150Z","shell.execute_reply.started":"2021-12-24T16:51:30.306337Z","shell.execute_reply":"2021-12-24T16:51:35.968411Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from cache_decorator import Cache\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom typing import Dict, List, Tuple, Optional, Union\n\nfrom keras_mixed_sequence import MixedSequence\n\nfrom pathlib import Path\nfrom datetime import datetime\nimport time\n\n\n@Cache(\n    cache_path=[\n        \"./model-histories/model_histories/{cell_line}/{task}/{model_name}/{use_feature_selection}/history_{_hash}.csv.xz\",\n        \"./model-performance/model_performance/{cell_line}/{task}/{model_name}/{use_feature_selection}/performance_{_hash}.csv.xz\"\n    ],\n    args_to_ignore=[\"model\", \"training_sequence\", \"test_sequence\"]\n)\ndef train_model(\n    model: Model,\n    model_name: str,\n    task: str,\n    cell_line: str,\n    training_sequence: MixedSequence,\n    test_sequence: MixedSequence,\n    holdout_number: int,\n    use_feature_selection: bool,\n    start_time: time\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n\n    \"\"\"Returns training history and model evaluations.\n    \n    Parameters\n    ---------------------\n    model: Model,\n        The model to train.\n    model_name: str,\n        The model name.\n    task: str,\n        The name of the task.\n    cell_line: str,\n        Name of the considered cell line.\n    training_sequence: MixedSequence,\n        The training sequence.\n    test_sequence: MixedSequence,\n        The test sequence.\n    holdout_number: int,\n        The number of the current holdout.\n    use_feature_selection: bool,\n        Whether the model is trained using features that have\n        been selected with Boruta or not.\n\n    Returns\n    ----------------------\n    Tuple with training history dataframe and model evaluations dataframe.\n    \"\"\"\n\n    history = pd.DataFrame(model.fit(\n        train_sequence,\n        validation_data=test_sequence,\n        epochs=1000,\n        verbose=False,\n        callbacks=[\n            EarlyStopping(\n                \"loss\",\n                min_delta=0.001,\n                patience=2,\n                mode=\"min\"\n            )\n            #TqdmCallback(verbose=1)\n        ]\n    ).history)\n    \n    train_evaluation = dict(zip(model.metrics_names, model.evaluate(train_sequence, verbose=False)))\n    test_evaluation = dict(zip(model.metrics_names, model.evaluate(test_sequence, verbose=False)))\n    train_evaluation[\"run_type\"] = \"train\"\n    test_evaluation[\"run_type\"] = \"test\"\n    \n    for evaluation in (train_evaluation, test_evaluation):\n        evaluation[\"model_name\"] = model_name\n        evaluation[\"task\"] = task\n        evaluation[\"holdout_number\"] = holdout_number \n        evaluation[\"use_feature_selection\"] = use_feature_selection\n        evaluation[\"elapsed_time\"] = round(time.time() - start_time, 2)\n\n    evaluations = pd.DataFrame([\n        train_evaluation,\n        test_evaluation\n    ])\n    \n    return history, evaluations\n","metadata":{"id":"OP0tWQ-RXXXv","executionInfo":{"status":"ok","timestamp":1640104429260,"user_tz":-60,"elapsed":9,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T16:51:35.970582Z","iopub.execute_input":"2021-12-24T16:51:35.970853Z","iopub.status.idle":"2021-12-24T16:51:35.985343Z","shell.execute_reply.started":"2021-12-24T16:51:35.970818Z","shell.execute_reply":"2021-12-24T16:51:35.984477Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from ucsc_genomes_downloader import Genome\n\ngenome = Genome(\"hg38\", cache_directory=GENOME_CACHE_DIR)","metadata":{"id":"m59SMAHryRmB","executionInfo":{"status":"ok","timestamp":1640104514621,"user_tz":-60,"elapsed":83516,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"outputId":"9306f47f-aa74-4afd-c101-6ac0be3fd0c3","execution":{"iopub.status.busy":"2021-12-24T16:51:35.987530Z","iopub.execute_input":"2021-12-24T16:51:35.987942Z","iopub.status.idle":"2021-12-24T17:00:20.972275Z","shell.execute_reply.started":"2021-12-24T16:51:35.987906Z","shell.execute_reply":"2021-12-24T17:00:20.971418Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n\nholdouts_generator = StratifiedShuffleSplit(\n    n_splits=HOLDOUTS_NUM_SPLIT,\n    test_size=TEST_SIZE\n)","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1640104514623,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"},"user_tz":-60},"id":"rjXiVpBXFl7e","execution":{"iopub.status.busy":"2021-12-24T17:00:20.973995Z","iopub.execute_input":"2021-12-24T17:00:20.974882Z","iopub.status.idle":"2021-12-24T17:00:20.980495Z","shell.execute_reply.started":"2021-12-24T17:00:20.974821Z","shell.execute_reply":"2021-12-24T17:00:20.979778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras import optimizers\nfrom extra_keras_metrics import get_complete_binary_metrics\nfrom keras_tuner import HyperModel\n\n\nclass FFNNHyperModel(HyperModel):\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n\n    def build(self, hp):\n        num_layers = hp.Int(name=\"num_layers\", min_value=2, max_value=6)\n        n_neurons0 = hp.Int(name=\"n_neurons0\", min_value=32, max_value=256, step=32)\n        learning_rate = hp.Choice(name=\"learning_rate\", values=[1e-2, 1e-4])\n\n        input_epigenomic_data = Input((self.input_shape,), name=\"input_epigenomic_data\")\n        last_hidden_ffnn = hidden = input_epigenomic_data\n\n        for layer in range(num_layers):\n            if layer == (num_layers - 1):\n                name = \"last_hidden_ffnn\"\n            else:\n                name = None\n            if layer >= 2:\n                with hp.conditional_scope(\"num_layers\", [3, 4, 5, 6, 6]):\n                    n_neurons1 = hp.Int(\n                        name=\"n_neurons1\",\n                        min_value=16,\n                        max_value=256,\n                        step=16\n                    )\n\n                    hidden = Dense(\n                        n_neurons1,\n                        activation=\"relu\",\n                        kernel_regularizer=None,\n                        name=name\n                    )(hidden)\n                    last_hidden_ffnn = hidden\n            else:\n                hidden = Dense(\n                    n_neurons0,\n                    activation=\"relu\",\n                    kernel_regularizer=None,\n                    name=name\n                )(hidden)\n                last_hidden_ffnn = hidden\n\n        output_ffnn = Dense(1, activation=\"sigmoid\")(last_hidden_ffnn)\n\n        model = Model(inputs=input_epigenomic_data, outputs=output_ffnn, name=FFNN_NAME_HP)\n\n        model.compile(\n            loss=\"binary_crossentropy\",\n            optimizer=optimizers.Nadam(learning_rate=learning_rate),\n            metrics=get_complete_binary_metrics()\n        )\n\n        return model, input_epigenomic_data, last_hidden_ffnn\n","metadata":{"id":"0TGk57WbW0LR","executionInfo":{"status":"ok","timestamp":1640104515067,"user_tz":-60,"elapsed":454,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:00:20.982000Z","iopub.execute_input":"2021-12-24T17:00:20.982505Z","iopub.status.idle":"2021-12-24T17:00:21.127970Z","shell.execute_reply.started":"2021-12-24T17:00:20.982464Z","shell.execute_reply":"2021-12-24T17:00:21.127257Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, ReLU\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, GlobalAveragePooling1D\nfrom tensorflow.keras import optimizers\nfrom extra_keras_metrics import get_complete_binary_metrics\nfrom keras_tuner import HyperModel\n\n\nclass CNNHyperModel(HyperModel):\n    def __init__(self, window_size):\n        self.window_size = window_size\n\n    def build(self, hp):\n        num_conv_layers = hp.Int(name=\"num_conv_layers\", min_value=2, max_value=8, step=1)\n        n_neurons0 = hp.Int(name=\"n_neurons0\", min_value=32, max_value=128, step=32)\n        kernel_size0 = hp.Int(name=\"kernel_size0\", min_value=5, max_value=8)\n        drop_rate = hp.Float(name=\"drop_rate\", min_value=0.0, max_value=0.5)\n        drop_rate_out = hp.Float(name=\"drop_rate_out\", min_value=0.0, max_value=0.5)\n        learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-4])\n        n_neurons_last_out = hp.Int(name=\"n_neurons_last_out\", min_value=16, max_value=128, step=16)\n\n        input_sequence_data = Input((self.window_size, 4), name=\"input_sequence_data\")\n        hidden = input_sequence_data\n\n        for num_conv_layer in range(num_conv_layers):\n            if num_conv_layer >= 2:\n                with hp.conditional_scope(\"num_conv_layers\", [3, 4, 5, 6, 7, 8]):\n                    n_neurons1 = hp.Int(name=\"n_neurons1\", min_value=16, max_value=128, step=16)\n                    kernel_size1 = hp.Int(name=\"kernel_size1\", min_value=2, max_value=10)\n\n                    hidden = Conv1D(n_neurons1, kernel_size=kernel_size1)(hidden)\n                    hidden = BatchNormalization()(hidden)\n                    hidden = ReLU()(hidden)\n                    hidden = Dropout(drop_rate)(hidden)\n            else:\n                hidden = Conv1D(n_neurons0, kernel_size=kernel_size0)(hidden)\n                hidden = BatchNormalization()(hidden)\n                hidden = ReLU()(hidden)\n                hidden = Dropout(drop_rate)(hidden)\n\n            if num_conv_layer % 2 != 0:\n                hidden = MaxPool1D(pool_size=2, padding='same')(hidden)\n\n        hidden = GlobalAveragePooling1D()(hidden)\n        hidden = BatchNormalization()(hidden)\n        hidden = Dropout(drop_rate_out)(hidden)\n        last_hidden_cnn = Dense(n_neurons_last_out, activation=\"relu\", name=\"last_hidden_cnn\")(hidden)\n        output_cnn = Dense(1, activation=\"sigmoid\", name=\"output_cnn\")(last_hidden_cnn)\n\n        model = Model(inputs=input_sequence_data, outputs=output_cnn, name=CNN_NAME_HP)\n\n        model.compile(\n            loss=\"binary_crossentropy\",\n            optimizer=optimizers.Nadam(learning_rate=learning_rate),\n            metrics=get_complete_binary_metrics()\n        )\n\n        return model, input_sequence_data, last_hidden_cnn\n","metadata":{"id":"uzc0GUs_Wdtc","executionInfo":{"status":"ok","timestamp":1640104515068,"user_tz":-60,"elapsed":9,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:00:21.129471Z","iopub.execute_input":"2021-12-24T17:00:21.129741Z","iopub.status.idle":"2021-12-24T17:00:21.147723Z","shell.execute_reply.started":"2021-12-24T17:00:21.129705Z","shell.execute_reply":"2021-12-24T17:00:21.146931Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Concatenate\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom extra_keras_metrics import get_complete_binary_metrics\nfrom keras_tuner import HyperModel\n\n\nclass MMNNHyperModel(HyperModel):\n    def __init__(self, input_epigenomic_data, input_sequence_data, last_hidden_ffnn, last_hidden_cnn):\n        self.input_epigenomic_data = input_epigenomic_data\n        self.input_sequence_data = input_sequence_data\n        self.last_hidden_ffnn = last_hidden_ffnn\n        self.last_hidden_cnn = last_hidden_cnn\n\n    def build(self, hp):\n        n_neurons_concat = hp.Int(name=\"n_neurons_concat\", min_value=32, max_value=256, step=32)\n        learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-4])\n\n        concatenation_layer = Concatenate()([self.last_hidden_ffnn, self.last_hidden_cnn])\n\n        last_hidden_mmnn = Dense(n_neurons_concat, activation=\"relu\", name=\"First_Hidden_Layer\")(concatenation_layer)\n        output_mmnn = Dense(1, activation=\"sigmoid\", name=\"Output_Layer\")(last_hidden_mmnn)\n\n        model = Model(inputs=[self.input_epigenomic_data, self.input_sequence_data], outputs=output_mmnn, name=MMNN_NAME_HP)\n        model.compile(\n            optimizer=optimizers.Nadam(learning_rate=learning_rate),\n            loss=\"binary_crossentropy\",\n            metrics=get_complete_binary_metrics()\n        )\n\n        return model\n","metadata":{"id":"6TBIW1KwWwfm","executionInfo":{"status":"ok","timestamp":1640104515070,"user_tz":-60,"elapsed":9,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:00:21.149024Z","iopub.execute_input":"2021-12-24T17:00:21.149455Z","iopub.status.idle":"2021-12-24T17:00:21.161676Z","shell.execute_reply.started":"2021-12-24T17:00:21.149416Z","shell.execute_reply":"2021-12-24T17:00:21.160902Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from ucsc_genomes_downloader import Genome\n\nfrom typing import Tuple\n\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom loguru import logger\nfrom typing import Optional\n\nfrom keras_tuner import Hyperband\nimport keras_tuner as kt\n\nimport numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nimport datetime\n\n\ndef hyperparameter_tuning(\n        train_X: np.ndarray,\n        test_X: np.ndarray,\n        train_y: np.ndarray,\n        test_y: np.ndarray,\n        train_bed: pd.DataFrame,\n        test_bed: pd.DataFrame,\n        genome: Genome,\n        window_size: int,\n        holdout_number: int,\n        task_name: str,\n        model_name: str,\n        input_layers: Optional[list] = None,\n        hidden_layers: Optional[list] = None\n)-> Tuple[Model, Layer, Layer]:\n    \"\"\"Returns tuple with list of kept features and list of discared features.\n\n    Parameters\n    --------------------------\n    train_X: np.ndarray,\n        The vector from where to extract the epigenomic. Used for training phase.\n    test_X: np.ndarray,\n        The vector from where to extract the epigenomic. Used for test phase.\n    train_y: np.ndarray,\n        The values the model should predict during the training phase.\n    test_y: np.ndarray,\n        The values the model should predict during the test phase.\n    holdout_number: int,\n        The current holdout number.\n    task_name: str,\n        The name of the task.\n    model_name: str,\n        The name of the model.\n\n    Returns\n    -------\n    Return FFNN model resulting from hyperparameter optimization, and dictionary\n    that contains best hyperparamters results.\n    \"\"\"\n\n    global hyperparam_results, max_epochs, project_name, directory\n\n    task_name = \"AEvsIE\" if task_name == \"active_enhancers_vs_inactive_enhancers\" else \"APvsIP\"\n\n    if model_name == MODEL_TYPE_FFNN:\n        hypermodel = FFNNHyperModel(input_shape=train_X.shape[1])\n        max_epochs = HP_MAX_EPOCHS_FFNN\n        project_name = TUNER_PROJECT_NAME_FFNN\n        directory = TUNER_DIR_FFNN\n    elif model_name == MODEL_TYPE_CNN:\n        hypermodel = CNNHyperModel(window_size)\n        max_epochs = HP_MAX_EPOCHS_CNN\n        project_name = TUNER_PROJECT_NAME_CNN\n        directory = TUNER_DIR_CNN\n    elif model_name == MODEL_TYPE_MMNN:\n        if input_layers and hidden_layers:\n            #print(f\"model_name: {model_name} input_layers: {input_layers} hidden_layers: {hidden_layers}\")\n            #print(input_layers.get(\"input_epigenomic_data\"))\n            hypermodel = MMNNHyperModel(input_layers.get(\"input_epigenomic_data\"),\n                                        input_layers.get(\"input_sequence_data\"),\n                                        hidden_layers.get(\"last_hidden_ffnn\"),\n                                        hidden_layers.get(\"last_hidden_cnn\")\n                                        )\n        max_epochs = HP_MAX_EPOCHS_MMNN\n        project_name = TUNER_PROJECT_NAME_MMNN\n        directory = TUNER_DIR_MMNN\n\n    tuners = define_tuners(hypermodel,\n                           max_epochs=max_epochs,\n                           directory=directory,\n                           project_name=project_name)\n\n    for tuner in tuners:\n        hyperparam_results = tuner_evaluation(tuner,\n                                              train_X,\n                                              test_X,\n                                              train_y,\n                                              test_y,\n                                              train_bed,\n                                              test_bed,\n                                              genome,\n                                              task_name,\n                                              holdout_number,\n                                              model_name)\n\n    return hyperparam_results\n\n\ndef tuner_evaluation(tuner, train_X, test_X, train_y, test_y, train_bed, test_bed, genome, task_name, holdout_number, model_name):\n    # Overview of the task\n    # tuner.search_space_summary()\n\n    # Performs the hyperparameter tuning\n    global train_search_seq, valid_search_seq\n    model = None\n\n    #logger.info(f\"Start hyperparameter tuning for {model_name}\")\n\n    if model_name == MODEL_TYPE_FFNN:\n        train_search_seq = get_ffnn_sequence(train_X, train_y)\n        valid_search_seq = get_ffnn_sequence(test_X, test_y)\n    elif model_name == MODEL_TYPE_CNN:\n        train_search_seq = get_cnn_sequence(genome, train_bed, train_y)\n        valid_search_seq = get_cnn_sequence(genome, test_bed, test_y)\n    elif model_name == MODEL_TYPE_MMNN:\n        train_search_seq = get_mmnn_sequence(genome, train_bed, train_X, train_y)\n        valid_search_seq = get_mmnn_sequence(genome, test_bed, test_X, test_y)\n\n    tuner.search(train_search_seq,\n                 validation_data=valid_search_seq,\n                 epochs=200,\n                 batch_size=256,\n                 callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1)]\n                 )\n\n    # Show a summary of the search\n    # tuner.results_summary()\n\n    # Retrieve the best hyperparameters.\n    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n    model = tuner.hypermodel.build(best_hps)\n\n    if model_name == MODEL_TYPE_FFNN:\n        #print(f\"Get Layer From {model_name} Models!\")\n        input_epigenomic_data = model[1]\n        last_hidden_ffnn = model[2]\n\n    if model_name == MODEL_TYPE_CNN:\n        #print(f\"Get Layer From {model_name} Models!\")\n        input_sequence_data = model[1]\n        last_hidden_cnn = model[2]\n\n    results = {\n        \"task_name\": task_name,\n        \"holdout_number\": holdout_number,\n        \"learning_rate\": best_hps.get(\"learning_rate\"),\n        \"create_date\": datetime.datetime.now().strftime(\"%d/%m/%Y-%H:%M:%S\")\n    }\n\n    if model_name == MODEL_TYPE_FFNN:\n        results[\"num_layers\"] = best_hps.get(\"num_layers\")\n        results[\"n_neurons0\"] = best_hps.get(\"n_neurons0\")\n        results[\"n_neurons1\"] = best_hps.get(\"n_neurons1\") if best_hps.get(\"num_layers\") >= 3 else None\n        results[\"input_epigenomic_data\"] = input_epigenomic_data\n        results[\"last_hidden_ffnn\"] = last_hidden_ffnn\n\n    if model_name == MODEL_TYPE_CNN:\n        results[\"num_layers\"] = best_hps.get(\"num_conv_layers\")\n        results[\"n_neurons0\"] = best_hps.get(\"n_neurons0\")\n        results[\"kernel_size0\"] = best_hps.get(\"kernel_size0\")\n        results[\"drop_rate\"] = best_hps.get(\"drop_rate\")\n        results[\"n_neurons1\"] = best_hps.get(\"n_neurons1\") if best_hps.get(\"num_conv_layers\") >= 3 else None\n        results[\"kernel_size1\"] = best_hps.get(\"kernel_size1\") if best_hps.get(\"num_conv_layers\") >= 3 else None\n        results[\"n_neurons_last_out\"] = best_hps.get(\"n_neurons_last_out\")\n        results[\"drop_rate_out\"] = best_hps.get(\"drop_rate_out\")\n        results[\"input_sequence_data\"] = input_sequence_data\n        results[\"last_hidden_cnn\"] = last_hidden_cnn\n\n    if model_name == MODEL_TYPE_MMNN:\n        results[\"n_neurons_concat\"] = best_hps.get(\"n_neurons_concat\")\n\n    return {model_name: model, f\"{model_name}_parameters\": results}\n\n\ndef define_tuners(hypermodel, max_epochs, directory, project_name):\n    hyperband_tuner = Hyperband(\n        hypermodel,\n        max_epochs=max_epochs,\n        objective=kt.Objective(\"val_AUPRC\", direction=\"max\"),\n        factor=3,\n        directory=directory,\n        project_name=project_name\n    )\n    return [hyperband_tuner]\n","metadata":{"id":"vdpleMXVW-oX","executionInfo":{"status":"ok","timestamp":1640104515516,"user_tz":-60,"elapsed":454,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:00:21.163881Z","iopub.execute_input":"2021-12-24T17:00:21.164161Z","iopub.status.idle":"2021-12-24T17:00:21.204894Z","shell.execute_reply.started":"2021-12-24T17:00:21.164124Z","shell.execute_reply":"2021-12-24T17:00:21.204192Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom epigenomic_dataset import active_enhancers_vs_inactive_enhancers, active_promoters_vs_inactive_promoters\nimport warnings\nwarnings.filterwarnings('ignore')\n\nall_tuner_results = []\nall_input_layer = []\nall_output_layer = []\n\nfor task, threshold in tqdm((\n    (active_enhancers_vs_inactive_enhancers, 0),\n    (active_promoters_vs_inactive_promoters, 1)\n), desc=\"Tasks\"):\n    all_results = []\n    task_name = task.__name__\n\n    # We get the task data with binarized labels\n    X, y = task(\n        binarize=True,\n        cell_line=CELL_LINE,\n        window_size=WINDOW_SIZE,\n        min_active_tpm_value=threshold,\n        max_inactive_tpm_value=threshold,\n        root=\"./bio_data/epigenomic/\"+str(task_name),\n        verbose=1\n    )\n    bed = to_bed(X)\n\n    # Start the main loop, iterating through the holdouts\n    for holdout_number, (train_indices, test_indices) in tqdm(\n        enumerate(holdouts_generator.split(X, y)),\n        total=HOLDOUTS_NUM_SPLIT,\n        leave=False,\n        desc=\"Computing Holdouts For {}\".format(task_name)\n    ):\n        # Get the training and test data\n        train_bed, test_bed = bed.iloc[train_indices], bed.iloc[test_indices]\n        train_X, test_X = X.iloc[train_indices], X.iloc[test_indices]\n        train_y, test_y = y.iloc[train_indices], y.iloc[test_indices]\n\n        # Impute and normalize the epigenomic data\n        train_X, test_X = normalize_epigenomic_data(train_X, test_X)\n\n        # Flatten the output values\n        train_y = train_y.values.flatten()\n        test_y = test_y.values.flatten()\n\n        input_layers = {}\n        output_layers = {}\n        for model in MODELS_TYPE:\n            if model == MODEL_TYPE_MMNN:\n                check_ffn_param = any(\"ffnn_parameters\" in d for d in all_tuner_results[:2])\n                check_cnn_param = any(\"cnn_parameters\" in d for d in all_tuner_results[:2])\n                if check_ffn_param and check_cnn_param:\n                    input_layers[\"input_epigenomic_data\"] = all_tuner_results[0].get(\"ffnn_parameters\").get(\"input_epigenomic_data\")\n                    input_layers[\"input_sequence_data\"] = all_tuner_results[1].get(\"cnn_parameters\").get(\"input_sequence_data\")\n                    output_layers[\"last_hidden_ffnn\"] = all_tuner_results[0].get(\"ffnn_parameters\").get(\"last_hidden_ffnn\")\n                    output_layers[\"last_hidden_cnn\"] = all_tuner_results[1].get(\"cnn_parameters\").get(\"last_hidden_cnn\")\n                    #print(f\"check_ffn_param: {check_ffn_param} check_cnn_param: {check_cnn_param} input_layers: {input_layers} output_layers: {output_layers} \")\n            tuner_result = hyperparameter_tuning(train_X,\n                                                test_X,\n                                                train_y,\n                                                test_y,\n                                                train_bed,\n                                                test_bed,\n                                                genome,\n                                                WINDOW_SIZE,\n                                                holdout_number,\n                                                task_name, model,\n                                                input_layers,\n                                                output_layers)\n            all_tuner_results.append(tuner_result)","metadata":{"id":"meAKKZQhFlOh","executionInfo":{"status":"ok","timestamp":1640105277246,"user_tz":-60,"elapsed":761745,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"outputId":"060be6e9-cf24-4d14-955c-2d9ca204e74e","execution":{"iopub.status.busy":"2021-12-24T17:00:21.206384Z","iopub.execute_input":"2021-12-24T17:00:21.206674Z","iopub.status.idle":"2021-12-24T17:10:53.644215Z","shell.execute_reply.started":"2021-12-24T17:00:21.206622Z","shell.execute_reply":"2021-12-24T17:10:53.643522Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_tuner_results[:3]","metadata":{"id":"uaq1BebDdMZ6","executionInfo":{"status":"ok","timestamp":1640105277247,"user_tz":-60,"elapsed":16,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"outputId":"770db750-ed4b-4903-9587-65163f16326c","execution":{"iopub.status.busy":"2021-12-24T17:10:53.645715Z","iopub.execute_input":"2021-12-24T17:10:53.646124Z","iopub.status.idle":"2021-12-24T17:10:53.656794Z","shell.execute_reply.started":"2021-12-24T17:10:53.646084Z","shell.execute_reply":"2021-12-24T17:10:53.656094Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\nimport silence_tensorflow.auto\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Layer\nfrom tensorflow.keras import optimizers\nfrom extra_keras_metrics import get_complete_binary_metrics\n\n\ndef build_binary_classification_ffnn(\n    input_shape: int,\n    hp_param: dict()\n) -> Tuple[Model, Layer, Layer]:\n    \"\"\"Build a custom Feed-Forward Neural Network.\n\n    Parameters\n    ----------\n    input_shape: int,\n        Number of features in the input layer.\n    hp_param : dict\n        Dictionary with best hyperparameters used for buil net.\n\n    Returns\n    -------\n    The compiled FFNN.\n    \"\"\"\n\n    num_layers = hp_param.get('num_layers')\n    n_neurons0 = hp_param.get('n_neurons0')\n    n_neurons1 = hp_param.get('n_neurons1')\n    learning_rate = hp_param.get('learning_rate')\n\n    input_epigenomic_data = Input(shape=(input_shape,), name=\"input_epigenomic_data\")\n    last_hidden_ffnn = hidden = input_epigenomic_data\n\n    for layer in range(num_layers):\n        if layer == (num_layers - 1):\n            name = \"last_hidden_ffnn\"\n        else:\n            name = None\n        if layer >= 2:\n            hidden = Dense(\n                n_neurons1,\n                activation=\"relu\",\n                kernel_regularizer=None,\n                name=name\n            )(hidden)\n            last_hidden_ffnn = hidden\n        else:\n            hidden = Dense(\n                n_neurons0,\n                activation=\"relu\",\n                kernel_regularizer=None,\n                name=name\n            )(hidden)\n            last_hidden_ffnn = hidden\n\n    output_ffnn = Dense(1, activation=\"sigmoid\")(last_hidden_ffnn)\n\n    ffnn = Model(inputs=input_epigenomic_data,\n                  outputs=output_ffnn,\n                  name=FFNN_NAME)\n\n    ffnn.compile(\n        optimizer=optimizers.Nadam(learning_rate=learning_rate),\n        loss=\"binary_crossentropy\",\n        metrics=get_complete_binary_metrics()\n    )\n\n    return ffnn, input_epigenomic_data, last_hidden_ffnn","metadata":{"id":"ObfSGZOfZEsk","executionInfo":{"status":"ok","timestamp":1640105277248,"user_tz":-60,"elapsed":11,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:10:53.658128Z","iopub.execute_input":"2021-12-24T17:10:53.658559Z","iopub.status.idle":"2021-12-24T17:10:53.680648Z","shell.execute_reply.started":"2021-12-24T17:10:53.658515Z","shell.execute_reply":"2021-12-24T17:10:53.679921Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\nimport silence_tensorflow.auto\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, MaxPool1D, GlobalAveragePooling1D\nfrom tensorflow.keras import optimizers\nfrom tensorflow.python.layers.base import Layer\nfrom extra_keras_metrics import get_complete_binary_metrics\n\n\ndef build_binary_classification_cnn(\n        window_size: int,\n        hp_param: dict()\n) -> Tuple[Model, Layer, Layer]:\n    \"\"\"Returns Convolutional Neural Network model for binary classification.\n\n    Parameters\n    -----------------------\n    window_size: int,\n        Size of the input genomic window.\n\n    Returns\n    -----------------------\n    Triple with model, input layer and output layer.\n    \"\"\"\n\n    num_layers = hp_param.get(\"num_layers\")\n    n_neurons0 = hp_param.get(\"n_neurons0\")\n    kernel_size0 = hp_param.get(\"kernel_size0\")\n\n    n_neurons1 = hp_param.get(\"n_neurons1\")\n    kernel_size1 = hp_param.get(\"kernel_size1\")\n\n    drop_rate = hp_param.get(\"drop_rate\")\n\n    n_neurons_last_out = hp_param.get(\"n_neurons_last_out\")\n    drop_rate_out = hp_param.get(\"drop_rate_out\")\n\n    learning_rate = hp_param.get(\"learning_rate\")\n\n    input_sequence_data = Input(shape=(window_size, 4), name=\"input_sequence_data\")\n    hidden = Conv1D(n_neurons0, kernel_size=kernel_size0, activation=\"relu\")(input_sequence_data)\n\n    for _ in range(num_layers):\n        hidden = Conv1D(\n            n_neurons1,\n            kernel_size=kernel_size1,\n            activation=\"relu\",\n        )(hidden)\n        hidden = Dropout(rate=drop_rate)(hidden)\n        hidden = MaxPool1D(pool_size=2, padding='same')(hidden)\n\n    hidden = GlobalAveragePooling1D()(hidden)\n    hidden = BatchNormalization()(hidden)\n    hidden = Dropout(rate=drop_rate_out)(hidden)\n    last_hidden_cnn = Dense(n_neurons_last_out, activation=\"relu\")(hidden)\n    output_cnn = Dense(1, activation=\"sigmoid\")(last_hidden_cnn)\n\n    cnn = Model(\n        inputs=input_sequence_data,\n        outputs=output_cnn,\n        name=CNN_NAME\n    )\n\n    cnn.compile(\n        optimizer=optimizers.Nadam(learning_rate=learning_rate),\n        loss=\"binary_crossentropy\",\n        metrics=get_complete_binary_metrics()\n    )\n    return cnn, input_sequence_data, last_hidden_cnn","metadata":{"id":"43OjSxSNZIQ_","executionInfo":{"status":"ok","timestamp":1640105277249,"user_tz":-60,"elapsed":11,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:10:53.684473Z","iopub.execute_input":"2021-12-24T17:10:53.684799Z","iopub.status.idle":"2021-12-24T17:10:53.700186Z","shell.execute_reply.started":"2021-12-24T17:10:53.684767Z","shell.execute_reply":"2021-12-24T17:10:53.699395Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from typing import Optional\nimport silence_tensorflow.auto\nfrom tensorflow.keras.layers import Dense, Concatenate\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.python.layers.base import Layer\n\nfrom extra_keras_metrics import get_complete_binary_metrics\n\n\ndef build_binary_classification_mmnn(\n        hp_param_mmnn: dict(),\n        hp_param_ffnn: Optional[dict] = None,\n        hp_param_cnn: Optional[dict] = None,\n        input_shape: Optional[int] = None,\n        window_size: Optional[int] = None,\n        input_epigenomic_data: Optional[Layer] = None,\n        input_sequence_data: Optional[Layer] = None,\n        last_hidden_ffnn: Optional[Layer] = None,\n        last_hidden_cnn: Optional[Layer] = None,\n) -> Model:\n    \"\"\"Returns Multi-Modal Neural Network model for binary classification.\n\n    Implementative details\n    -----------------------\n    If the input shape / window size is not provided and the input layers and\n    the feature selection layers are provided, then the network will start\n    to train from those layers (which are expected to be pre-trained).\n    Conversely, it will create the submodules for the epigenomic and sequence\n    data ex-novo.\n\n    Parameters\n    -----------------------\n    input_shape: Optional[int] = None,\n        Number of features in the input layer.\n        Either the input shape or the input and output layers of the FFNN\n        must be provided.\n    window_size: int,\n        Size of the input genomic window.\n        Either the window size or the input and output layers of the CNN\n        must be provided.\n    input_epigenomic_data: Optional[Layer] = None,\n        Input for the epigenomic data from a FFNN model.\n        Either the input shape or the input and output layers of the FFNN\n        must be provided.\n    input_sequence_data: Optional[Layer] = None,\n        Input for the sequence data from a CNN model.\n        Either the window size or the input and output layers of the CNN\n        must be provided.\n    last_hidden_ffnn: Optional[Layer] = None,\n        Feature selection layer from a FFNN model.\n        Either the input shape or the input and output layers of the FFNN\n        must be provided.\n    last_hidden_cnn: Optional[Layer] = None,\n        Feature selection layer from a CNN model.\n        Either the window size or the input and output layers of the CNN\n        must be provided.\n\n    Raises\n    -----------------------\n    ValueError,\n        If the input shape is not provided and the input layer and feature selection\n        layer of the FFNN are not provided.\n    ValueError,\n        If the window size is not provided and the input layer and feature selection\n        layer of the CNN are not provided.\n\n    Returns\n    -----------------------\n    Triple with model, input layer and output layer.\n    \"\"\"\n\n    learning_rate = hp_param_mmnn.get(\"learning_rate\")\n    n_neurons_concat = hp_param_mmnn.get(\"n_neurons_concat\")\n\n    if input_shape is None and (last_hidden_ffnn is None or input_epigenomic_data is None):\n        raise ValueError(\n            \"Either the input shape or the features selection layer and the input epigenomic \"\n            \"layer must be provided.\"\n        )\n    if window_size is None and (last_hidden_cnn is None or input_sequence_data is None):\n        raise ValueError(\n            \"Either the input shape or the features selection layer and the input sequence \"\n            \"layer must be provided.\"\n        )\n\n    if input_shape is not None and hp_param_ffnn is not None:\n        _, input_epigenomic_data, last_hidden_ffnn = build_binary_classification_ffnn(input_shape, hp_param_ffnn)\n\n    if window_size is not None and hp_param_cnn is not None:\n        _, input_sequence_data, last_hidden_cnn = build_binary_classification_cnn(window_size, hp_param_cnn)\n\n    concatenation_layer = Concatenate()([\n        last_hidden_ffnn,\n        last_hidden_cnn\n    ])\n\n    last_hidden_mmnn = Dense(n_neurons_concat, activation=\"relu\", name=\"last_hidden_mmnn\")(concatenation_layer)\n    output_mmnn = Dense(1, activation=\"sigmoid\")(last_hidden_mmnn)\n\n    mmnn = Model(\n        inputs=[input_epigenomic_data, input_sequence_data],\n        outputs=output_mmnn,\n        name=MMNN_BOOST if input_shape is None else MMNN_SIMPLE\n    )\n\n    mmnn.compile(\n        optimizer=optimizers.Nadam(learning_rate=learning_rate),\n        loss=\"binary_crossentropy\",\n        metrics=get_complete_binary_metrics()\n    )\n\n    return mmnn","metadata":{"id":"1PFxaafkZK3P","executionInfo":{"status":"ok","timestamp":1640105277249,"user_tz":-60,"elapsed":11,"user":{"displayName":"Giuseppe Bonura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpg6yYD2jl5FC3Q18YRd8xIR88r8Qu4T2XUmowMA=s64","userId":"06482237288690829528"}},"execution":{"iopub.status.busy":"2021-12-24T17:10:53.701650Z","iopub.execute_input":"2021-12-24T17:10:53.702026Z","iopub.status.idle":"2021-12-24T17:10:53.717807Z","shell.execute_reply.started":"2021-12-24T17:10:53.701990Z","shell.execute_reply":"2021-12-24T17:10:53.717070Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom epigenomic_dataset import active_enhancers_vs_inactive_enhancers, active_promoters_vs_inactive_promoters\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create a list to store all the computed performance\nall_binary_classification_performance = []\n\ntraining_histories = {}\n\n# For each task\nfor task, threshold in tqdm((\n    (active_enhancers_vs_inactive_enhancers, 0),\n    (active_promoters_vs_inactive_promoters, 1)\n), desc=\"Tasks\"):\n    start_time = time.time()\n    task_name = task.__name__\n    # We get the task data with binarized labels\n    X, y = task(\n        binarize=True,\n        cell_line=CELL_LINE,\n        window_size=WINDOW_SIZE,\n        min_active_tpm_value=threshold,\n        max_inactive_tpm_value=threshold,\n        root=\"./bio_data/epigenomic/\"+str(task_name),\n        verbose=1\n    )\n    training_histories[task_name] = []\n\n    # Start the main loop, iterating through the holdouts\n    for holdout_number, (train_indices, test_indices) in tqdm(\n        enumerate(holdouts_generator.split(X, y)),\n        total=HOLDOUTS_NUM_SPLIT,\n        leave=False,\n        desc=\"Computing Holdouts For {}\".format(task_name)\n    ):\n\n        for use_feature_selection in tqdm((True, False), desc=\"Running Feature Selection For {}\".format(task_name), leave=False):\n            # Get the training and test data\n            train_bed, test_bed = bed.iloc[train_indices], bed.iloc[test_indices]\n            train_X, test_X = X.iloc[train_indices], X.iloc[test_indices]\n            train_y, test_y = y.iloc[train_indices], y.iloc[test_indices]\n\n            # Impute and normalize the epigenomic data\n            train_X, test_X = normalize_epigenomic_data(train_X, test_X)\n\n            # Flatten the output values\n            train_y = train_y.values.flatten()\n            test_y = test_y.values.flatten()\n\n            if use_feature_selection:\n                kept_features, discarded_features = execute_boruta_feature_selection(\n                    X_train=pd.DataFrame(train_X),\n                    y_train=train_y,\n                    holdout_number=holdout_number,\n                    task_name=task_name,\n                    max_iter=20\n                )\n\n                if len(kept_features) > 0:\n                    train_X = train_X[:,kept_features]\n                    test_X = test_X[:,kept_features]\n\n            # Get the number of features of this specific dataset\n            number_of_features = train_X.shape[1]\n            ffnn_parameters = all_tuner_results[0].get(\"ffnn_parameters\")\n            cnn_parameters = all_tuner_results[1].get(\"cnn_parameters\")\n            mmnn_parameters = all_tuner_results[2].get(\"mmnn_parameters\")\n\n            ffnn, input_epigenomic_data, last_hidden_ffnn = build_binary_classification_ffnn(input_shape=number_of_features, hp_param=ffnn_parameters)\n            \n            cnn, input_sequence_data, last_hidden_cnn = build_binary_classification_cnn(window_size=WINDOW_SIZE, hp_param=cnn_parameters)\n\n            mmnn_simple = build_binary_classification_mmnn(hp_param_ffnn=ffnn_parameters,\n                                                           hp_param_cnn=cnn_parameters,\n                                                           hp_param_mmnn=mmnn_parameters,\n                                                           input_shape=number_of_features,\n                                                           window_size=WINDOW_SIZE)\n            \n            mmnn_boosted = build_binary_classification_mmnn(\n                hp_param_mmnn=mmnn_parameters,\n                input_sequence_data=input_sequence_data,\n                input_epigenomic_data=input_epigenomic_data,\n                last_hidden_ffnn=last_hidden_ffnn,\n                last_hidden_cnn=last_hidden_cnn\n            )\n            \n            for model, train_sequence, test_sequence in tqdm(\n                (\n                    (ffnn, get_ffnn_sequence(train_X, train_y), get_ffnn_sequence(test_X, test_y)),\n                    (cnn, get_cnn_sequence(genome, train_bed, train_y), get_cnn_sequence(genome, test_bed, test_y)),\n                    (mmnn_simple, get_mmnn_sequence(genome, train_bed, train_X, train_y), get_mmnn_sequence(genome, test_bed, test_X, test_y)),\n                    (mmnn_boosted, get_mmnn_sequence(genome, train_bed, train_X, train_y), get_mmnn_sequence(genome, test_bed, test_X, test_y)),\n                ),\n                desc=\"Training models\",\n                leave=False\n            ):\n\n                # We compute the model performance\n                history, performance = train_model(\n                    model,\n                    model.name+\"V1\",\n                    task_name,\n                    CELL_LINE,\n                    train_sequence,\n                    test_sequence,\n                    holdout_number,\n                    use_feature_selection,\n                    start_time\n                )\n                training_histories[task_name].append(history)\n                # We chain the computed performance to the performance list\n                all_binary_classification_performance.append(performance)\n\n                start_time = time.time()\n\n# We convert the computed performance list into a DataFrame\nall_binary_classification_performance = pd.concat(all_binary_classification_performance)","metadata":{"pycharm":{"name":"#%%\n"},"id":"rDckC3H0KStk","outputId":"c9f2e385-874b-4ba2-ca8c-38706f0aa136","execution":{"iopub.status.busy":"2021-12-24T17:10:53.719040Z","iopub.execute_input":"2021-12-24T17:10:53.719378Z","iopub.status.idle":"2021-12-25T00:28:55.530025Z","shell.execute_reply.started":"2021-12-24T17:10:53.719340Z","shell.execute_reply":"2021-12-25T00:28:55.529078Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!zip -r model-performance.zip ./model-performance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r model-histories.zip ./model-histories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r boruta.zip ./boruta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r barplots.zip ./barplots","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import walk\nfor (dirpath, dirnames, filenames) in walk(\"./training_histories_pe_vs_pe\"):\n    print(\"Directory path: \", dirpath)\n    print(\"Folder name: \", dirnames)\n    print(\"File name: \", filenames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.mkdir(\"./training_histories_ae_vs_ie\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:59:17.700414Z","iopub.execute_input":"2021-12-25T00:59:17.700698Z","iopub.status.idle":"2021-12-25T00:59:17.704581Z","shell.execute_reply.started":"2021-12-25T00:59:17.700667Z","shell.execute_reply":"2021-12-25T00:59:17.703907Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"import os\nos.mkdir(\"./training_histories_pe_vs_pe\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:59:20.481315Z","iopub.execute_input":"2021-12-25T00:59:20.481607Z","iopub.status.idle":"2021-12-25T00:59:20.487705Z","shell.execute_reply.started":"2021-12-25T00:59:20.481560Z","shell.execute_reply":"2021-12-25T00:59:20.486787Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"for index, x in enumerate(training_histories[\"active_enhancers_vs_inactive_enhancers\"]):\n    x.to_csv(f\"./training_histories_ae_vs_ie/active_enhancers_vs_inactive_enhancers_{index}.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:59:54.778651Z","iopub.execute_input":"2021-12-25T00:59:54.778936Z","iopub.status.idle":"2021-12-25T00:59:54.964279Z","shell.execute_reply.started":"2021-12-25T00:59:54.778900Z","shell.execute_reply":"2021-12-25T00:59:54.963557Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"for index, x in enumerate(training_histories[\"active_promoters_vs_inactive_promoters\"]):\n    x.to_csv(f\"./training_histories_pe_vs_pe/active_promoters_vs_inactive_promoters{index}.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:00:49.964643Z","iopub.execute_input":"2021-12-25T01:00:49.965566Z","iopub.status.idle":"2021-12-25T01:00:50.124691Z","shell.execute_reply.started":"2021-12-25T01:00:49.965524Z","shell.execute_reply":"2021-12-25T01:00:50.123957Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"!zip -r training_histories_ae_vs_ie.zip ./training_histories_ae_vs_ie","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r training_histories_pe_vs_pe.zip ./training_histories_pe_vs_pe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\ncurrent_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nall_binary_classification_performance.to_csv(f\"all_binary_classification_performance_{current_timestamp}.csv\", index=False)","metadata":{"id":"DP2NZBGJ7USk","execution":{"iopub.status.busy":"2021-12-25T00:28:58.042860Z","iopub.execute_input":"2021-12-25T00:28:58.043089Z","iopub.status.idle":"2021-12-25T00:28:58.074180Z","shell.execute_reply.started":"2021-12-25T00:28:58.043060Z","shell.execute_reply":"2021-12-25T00:28:58.073467Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"all_binary_classification_performance","metadata":{"id":"-HeIw2ivwJyi","execution":{"iopub.status.busy":"2021-12-25T00:28:58.075574Z","iopub.execute_input":"2021-12-25T00:28:58.076411Z","iopub.status.idle":"2021-12-25T00:28:58.125345Z","shell.execute_reply.started":"2021-12-25T00:28:58.076369Z","shell.execute_reply":"2021-12-25T00:28:58.124559Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"all_binary_classification_performance[all_binary_classification_performance['run_type'] == 'test'].sort_values(by='AUPRC', ascending=False)","metadata":{"id":"f2vjOOsLMnsh","execution":{"iopub.status.busy":"2021-12-25T00:28:58.126606Z","iopub.execute_input":"2021-12-25T00:28:58.126952Z","iopub.status.idle":"2021-12-25T00:28:58.163722Z","shell.execute_reply.started":"2021-12-25T00:28:58.126908Z","shell.execute_reply":"2021-12-25T00:28:58.162577Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Slightly adapting the dataframe in order to visualiza it better\nall_binary_classification_performance[\"use_feature_selection\"] = [\n    \"Feature Selection\" if use_selection else \"No Feature Selection\"\n    for use_selection in all_binary_classification_performance[\"use_feature_selection\"]\n]\nall_performance = all_binary_classification_performance.drop(columns=[\"holdout_number\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:28:58.165191Z","iopub.execute_input":"2021-12-25T00:28:58.165542Z","iopub.status.idle":"2021-12-25T00:28:58.172949Z","shell.execute_reply.started":"2021-12-25T00:28:58.165503Z","shell.execute_reply":"2021-12-25T00:28:58.172179Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from plot_keras_history import plot_history\n\nplot_history(training_histories[\"active_enhancers_vs_inactive_enhancers\"][14], title=\"active_enhancers_vs_inactive_enhancers\", graphs_per_row=6)\nplot_history(training_histories[\"active_promoters_vs_inactive_promoters\"][31], title=\"active_promoters_vs_inactive_promoters\", graphs_per_row=6)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:28:58.174554Z","iopub.execute_input":"2021-12-25T00:28:58.175096Z","iopub.status.idle":"2021-12-25T00:29:12.520065Z","shell.execute_reply.started":"2021-12-25T00:28:58.175053Z","shell.execute_reply":"2021-12-25T00:29:12.519432Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from plot_keras_history import plot_history\n\nfor region, x in training_histories.items():\n  plot_history(training_histories[region], title=region, graphs_per_row=6)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:29:12.521516Z","iopub.execute_input":"2021-12-25T00:29:12.521973Z","iopub.status.idle":"2021-12-25T00:30:00.052370Z","shell.execute_reply.started":"2021-12-25T00:29:12.521938Z","shell.execute_reply":"2021-12-25T00:30:00.051648Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from barplots import barplots\n\ncolumn_to_filter = [\"model_name\", \"loss\", \"accuracy\", \"AUROC\", \"AUPRC\", \"use_feature_selection\", \"run_type\"]\nbarplots(\n    all_performance[column_to_filter],\n    groupby=[\"model_name\", \"use_feature_selection\", \"run_type\"],\n    orientation=\"horizontal\",\n    height=8,\n    bar_width=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:31:41.340912Z","iopub.execute_input":"2021-12-25T00:31:41.341179Z","iopub.status.idle":"2021-12-25T00:31:44.199459Z","shell.execute_reply.started":"2021-12-25T00:31:41.341151Z","shell.execute_reply":"2021-12-25T00:31:44.198672Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import wilcoxon\n\nfor model in all_binary_classification_performance.model_name.unique():\n    model_performance = all_performance[(all_performance.model_name == model) & (all_performance.run_type == \"test\")]\n    performance_with_feature_selection = model_performance[\n        all_performance.use_feature_selection == \"Feature Selection\"\n    ]\n    performance_without_feature_selection = model_performance[\n        all_performance.use_feature_selection == \"No Feature Selection\"\n    ]\n\n    for metric in (\"AUPRC\", \"AUROC\", \"accuracy\"):\n        print(\n            model,\n            metric,\n            wilcoxon(performance_with_feature_selection[metric], performance_without_feature_selection[metric])\n        )\n    print(\"=\"*100)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:31:48.229380Z","iopub.execute_input":"2021-12-25T00:31:48.229790Z","iopub.status.idle":"2021-12-25T00:31:48.328544Z","shell.execute_reply.started":"2021-12-25T00:31:48.229750Z","shell.execute_reply":"2021-12-25T00:31:48.326474Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import wilcoxon\n\nfor model in all_binary_classification_performance.model_name.unique():\n    model_performance = all_performance[(all_performance.model_name == model) & (all_performance.run_type == \"test\")]\n    performance_with_feature_selection = model_performance[\n        all_performance.use_feature_selection == \"Feature Selection\"\n    ]\n    performance_without_feature_selection = model_performance[\n        all_performance.use_feature_selection == \"No Feature Selection\"\n    ]\n\n    for metric in (\"AUPRC\", \"AUROC\", \"accuracy\"):\n        try:\n                _, p_value = wilcoxon(\n                    performance_with_feature_selection[metric],\n                    performance_without_feature_selection[metric])\n                # when p_value is less than the significance level, then the null\n                # hypothesis is rejected and the model with greater mean for the\n                # current metric is better than the other.\n                if p_value < 0.01:\n                    if performance_with_feature_selection.mean() > performance_without_feature_selection.mean():\n                        print(f\"{model} shows better results for {metric} with feature selection.\")\n                    else:\n                        print(f\"{model} shows better results for {metric} without feature selection.\")\n                else:\n                    print(f\"{model} shows statistically indistinguishiable results for {metric} with and without feature selection.\")\n        except ValueError:\n            # this situation occurs when the matric are exactly the same.\n            print(f\"{model} has the same values for {metric}\")\n    print(\"=\"*130)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T00:30:05.610898Z","iopub.status.idle":"2021-12-25T00:30:05.611892Z","shell.execute_reply.started":"2021-12-25T00:30:05.611640Z","shell.execute_reply":"2021-12-25T00:30:05.611666Z"},"trusted":true},"execution_count":null,"outputs":[]}]}